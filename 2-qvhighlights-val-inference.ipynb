{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df795771-eff0-4293-a927-4bc122238b5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "76 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.11/site-packages (24.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-25.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.2\n",
      "    Uninstalling pip-24.2:\n",
      "      Successfully uninstalled pip-24.2\n",
      "Successfully installed pip-25.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "apt update\n",
    "python -m pip install --upgrade pip\n",
    "python -m pip install --quiet pandas av protobuf sentencepiece huggingface_hub\n",
    "python -m pip install --quiet transformers av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "504204d2-8621-4128-ac47-a3e211eac906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install --quiet einops einops_exts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1672c588-14ba-4beb-a248-1a01b59d3a5d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Download datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53d48689-e4bb-4bc3-bad5-80303e9890c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3d2906e-3358-432f-a296-41767760bc4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2e4774d190455bae2410ac7ab8aa43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "videos.tar.gz:   0%|          | 0.00/17.6G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef855570b1ee42839822acb60764a009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "highlight_val_release.jsonl:   0%|          | 0.00/821k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'highlight_val_release.jsonl'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_hub_download(\n",
    "    'jwnt4/qvhighlights-val', repo_type='dataset',\n",
    "    filename='videos.tar.gz', local_dir='.'\n",
    ")\n",
    "hf_hub_download(\n",
    "    'jwnt4/qvhighlights-val', repo_type='dataset',\n",
    "    filename='highlight_val_release.jsonl', local_dir='.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f222b36b-8607-4d12-935e-5f10862bd435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2025-04-27 10:49:05--  https://raw.githubusercontent.com/jayleicn/moment_detr/refs/heads/main/standalone_eval/eval.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15107 (15K) [text/plain]\n",
      "Saving to: ‘eval.py’\n",
      "\n",
      "     0K .......... ....                                       100% 4.38M=0.003s\n",
      "\n",
      "2025-04-27 10:49:05 (4.38 MB/s) - ‘eval.py’ saved [15107/15107]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wget https://raw.githubusercontent.com/jayleicn/moment_detr/refs/heads/main/standalone_eval/eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5d618a2-300e-43eb-8347-8acca2b3cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir videos/\n",
    "tar -xzf videos.tar.gz -C videos/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90c70026-79e3-4e58-9ab0-02009ffc0da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('highlight_val_release.jsonl', 'r') as f_in:\n",
    "    lines = f_in.read().strip().split('\\n')\n",
    "val_jsonl = []\n",
    "for line in lines:\n",
    "    val_jsonl.append(line)\n",
    "\n",
    "val_jsonl2 = random.sample(val_jsonl, 250)\n",
    "val_jsonl2_str = '\\n'.join(val_jsonl2)\n",
    "with open('highlight_val_small.jsonl', 'w') as f_out:\n",
    "    f_out.write(val_jsonl2_str)\n",
    "\n",
    "val_jsonl = []\n",
    "for line in val_jsonl2:\n",
    "    val_jsonl.append(json.loads(line))\n",
    "with open('highlight_val_small.json', 'w') as f_out:\n",
    "    json.dump(val_jsonl, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c02b32-8ccd-4149-9a78-e90e79bc64ab",
   "metadata": {},
   "source": [
    "# xgen-mm try best prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afc44c24-4629-4bdd-8a95-ac4e066d39f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.io\n",
    "from huggingface_hub import hf_hub_download\n",
    "import math\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d65dc44-c081-4533-a59f-3169059d7fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qid': 2988,\n",
       " 'query': 'mum is talking with girls and putting shoes on girl.',\n",
       " 'duration': 150,\n",
       " 'vid': 'H3fhZxUC5M8_360.0_510.0',\n",
       " 'relevant_clip_ids': [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74],\n",
       " 'saliency_scores': [[2, 4, 3],\n",
       "  [4, 4, 4],\n",
       "  [4, 4, 3],\n",
       "  [3, 4, 3],\n",
       "  [3, 4, 3],\n",
       "  [2, 4, 3],\n",
       "  [2, 4, 3],\n",
       "  [2, 4, 3],\n",
       "  [2, 4, 3],\n",
       "  [2, 4, 4],\n",
       "  [2, 4, 4],\n",
       "  [2, 4, 3],\n",
       "  [2, 4, 3]],\n",
       " 'relevant_windows': [[124, 150]]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load small set (250 questions instead of 1.5k)\n",
    "\n",
    "with open('highlight_val_small.json', 'r') as f_in:\n",
    "    val_jsonl = json.load(f_in)\n",
    "\n",
    "val_jsonl[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64e72ca8-6bdf-4827-b0a7-63a5a008708b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d48f89c6384eb4ad62681e12d78e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForVision2Seq, AutoTokenizer, AutoImageProcessor, LogitsProcessor\n",
    "import torch\n",
    "\n",
    "model_name_or_path = \"Salesforce/xgen-mm-vid-phi3-mini-r-v1.5-128tokens-8frames\"\n",
    "model = AutoModelForVision2Seq.from_pretrained(model_name_or_path, trust_remote_code=True, torch_dtype=torch.float32)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True, use_fast=False, legacy=False)\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "tokenizer = model.update_special_tokens(tokenizer)\n",
    "\n",
    "model = model.to('cuda', dtype=torch.float32)  # even if the model is loaded in float32, some params can be downcasted to bfloat16 which wont work\n",
    "model.eval()\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.eos_token = \"<|end|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0d6a0d4-37ac-4271-b82d-8a49652bf310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 27 11:49:38 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.58.02              Driver Version: 555.58.02      CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:01:00.0 Off |                  Off |\n",
      "|  0%   54C    P0             70W /  450W |   17548MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f892027-7e71-47ac-b49d-d24cb94719c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are some params that is still in bfloat16 or fp16\n",
    "for name, p in model.named_parameters():\n",
    "    if p.dtype != torch.float32:\n",
    "        print(f\"{name:40} --> {p.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adf2ee77-0855-4240-b58c-0496b37f67bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template1 = '''You are given a {frame}-frame video. At which frame do you see the activity: \"{activity}\"? \\\n",
    "Answer only with comma separated numbers denoting the frame numbers. \\\n",
    "Do not provide any other explanations!'''\n",
    "\n",
    "prompt_template2 = '''You are given a {frame}-frame video. Provide a comma-separated numbers for frames that matches with the activity: \"{activity}\". \\\n",
    "Do not provide any other explanations.'''\n",
    "\n",
    "prompt_template3 = '''- Instruction: analyze and find the relevant frames of the given Query.\n",
    "- Output format: comma-separated of the relevant frame number (e.g. 1,5,6).\n",
    "- Constraint: Do not provide any other explanations!\n",
    "- Number of frames: {frame}.\n",
    "- Query: {activity}.\n",
    "Question: What frames are relevant with the given Query?'''\n",
    "\n",
    "prompt_template4 = '''You are given a {frame}-frame video. At which frames you see {activity}? \\\n",
    "Provide only frame numbers separated by comma. Example of output format: \"1,2,3,10\". Do not provide any other explanations!'''\n",
    "\n",
    "prompt_templates = [\n",
    "    prompt_template1, prompt_template2, prompt_template3, prompt_template4\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d910af2-dd97-4b1c-a3ed-dc84233b491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_frames(vframes, num_frames, fps):\n",
    "    frame_indice = np.linspace(int(num_frames/2), len(vframes) - int(num_frames/2), num_frames, dtype=int)\n",
    "    video = vframes[frame_indice]\n",
    "    video_list = []\n",
    "    for i in range(len(video)):\n",
    "        video_list.append(torchvision.transforms.functional.to_pil_image(video[i]))\n",
    "    timestamps = (frame_indice / fps).tolist() \n",
    "    return video_list, list(map(round, timestamps))\n",
    "\n",
    "def generate(messages, images):\n",
    "    # img_bytes_list = [base64.b64decode(image.encode(\"utf-8\")) for image in images]\n",
    "    # images = [Image.open(BytesIO(img_bytes)) for img_bytes in img_bytes_list]\n",
    "    image_sizes = [image.size for image in images]\n",
    "    # Similar operation in model_worker.py\n",
    "    image_tensor = [image_processor([img])[\"pixel_values\"].to(model.device, dtype=torch.float32) for img in images]\n",
    "\n",
    "    image_tensor = torch.stack(image_tensor, dim=1)\n",
    "    image_tensor = image_tensor.squeeze(2)\n",
    "    inputs = {\"pixel_values\": image_tensor}\n",
    "\n",
    "    full_conv = \"<|system|>\\nA chat between a curious user and an artificial intelligence assistant.\\\n",
    "    The assistant gives helpful, detailed, and polite answers to the user's questions regarding the video.<|end|>\\n\"\n",
    "    for msg in messages:\n",
    "        msg_str = \"<|{role}|>\\n{content}<|end|>\\n\".format(\n",
    "            role=msg[\"role\"], content=msg[\"content\"]\n",
    "        )\n",
    "        full_conv += msg_str\n",
    "\n",
    "    full_conv += \"<|assistant|>\\n\"\n",
    "    # print(full_conv)\n",
    "    language_inputs = tokenizer([full_conv], return_tensors=\"pt\")\n",
    "    for name, value in language_inputs.items():\n",
    "        language_inputs[name] = value.to(model.device)\n",
    "    inputs.update(language_inputs)\n",
    "    # print(inputs)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        generated_text = model.generate(\n",
    "            **inputs,\n",
    "            image_size=[image_sizes],\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            temperature=0.05,\n",
    "            do_sample=False,\n",
    "            max_new_tokens=1024,\n",
    "            top_p=None,\n",
    "            num_beams=1,\n",
    "        )\n",
    "\n",
    "    outputs = (\n",
    "        tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
    "        .split(\"<|end|>\")[0]\n",
    "        .strip()\n",
    "    )\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def predict(qvh, num_frames=20, prompt_template=1):\n",
    "    video_file = f\"videos/{qvh['vid']}.mp4\"\n",
    "    \n",
    "    vframes, _, info = torchvision.io.read_video(\n",
    "        filename=video_file, pts_unit=\"sec\", output_format=\"TCHW\"\n",
    "    )\n",
    "    fps = float(info['video_fps'])\n",
    "    total_frames = len(vframes)\n",
    "    images, timestamps = sample_frames(vframes, num_frames, fps)\n",
    "\n",
    "    prompt = \"\"\n",
    "    prompt = prompt + \"<image>\\n\"\n",
    "    # prompt = prompt + \"What's the main gist of the video ?\"\n",
    "    # prompt = prompt + \"Please describe the primary object or subject in the video, capturing their attributes, actions, positions, and movements.\"\n",
    "    prompt = prompt + prompt_templates[prompt_template-1].format(frame=num_frames, activity=qvh['query'])\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    generated = generate(messages, images)\n",
    "    return generated, timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a191f8-fabc-4d02-8d27-848cb23d5a01",
   "metadata": {},
   "source": [
    "## val_jsonl[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e4fe545-30c6-46ef-91c4-b481dfef9a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.05` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The activity \"A woman in a black dress giving a tour of her hotel room\" is seen at frame 19.\n",
      "[[96, 130]]\n"
     ]
    }
   ],
   "source": [
    "preds = predict(val_jsonl[2], prompt_template=1)\n",
    "print(preds[0])\n",
    "print(val_jsonl[2]['relevant_windows'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd5471f5-87f1-41a1-8b75-254e3701484e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n",
      "[[96, 130]]\n"
     ]
    }
   ],
   "source": [
    "preds = predict(val_jsonl[2], prompt_template=2)\n",
    "print(preds[0])\n",
    "print(val_jsonl[2]['relevant_windows'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "558ed5ae-550f-4d57-8cbd-3e1e2ee14b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The relevant frames are 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, and 20.\n",
      "[[96, 130]]\n"
     ]
    }
   ],
   "source": [
    "preds = predict(val_jsonl[2], prompt_template=3)\n",
    "print(preds[0])\n",
    "print(val_jsonl[2]['relevant_windows'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "923a407b-f9c0-4873-abfb-41f299891952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The woman in a black dress is giving a tour of her hotel room at frames 1, 2, 3, and 10.\n",
      "[[96, 130]]\n"
     ]
    }
   ],
   "source": [
    "preds = predict(val_jsonl[2], prompt_template=4)\n",
    "print(preds[0])\n",
    "print(val_jsonl[2]['relevant_windows'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3acaa55-063a-4d41-b583-27df9d18d7c9",
   "metadata": {},
   "source": [
    "## val_jsonl[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73e27de9-0c95-4615-bb88-218c7530a513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The activity \"Old man speaks next to plaques\" is seen at frame 18.\n",
      "[[112, 128]]\n",
      "10\n",
      "[[112, 128]]\n",
      "The relevant frames with the given Query are 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, and 20.\n",
      "[[112, 128]]\n",
      "The frames where Old man speaks next to plaques are 1, 2, 3, and 10.\n",
      "[[112, 128]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    preds = predict(val_jsonl[10], prompt_template=i+1)\n",
    "    print(preds[0])\n",
    "    print(val_jsonl[10]['relevant_windows'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04efc0f-91b3-49e9-aa2c-36d70044d090",
   "metadata": {},
   "source": [
    "## prompts from chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0dad808-91fe-49a5-bca9-653c0bbd5202",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompts = [\n",
    "'''Example: Video of 5 frames, activity “clap” → 1,3  \n",
    "You have a {frame}-frame video. Identify all frames with “{activity}”.  \n",
    "Strictly output only a comma-separated list of frame numbers. No other text.''',\n",
    "'''Example: 5-frame video, “sit” → 2,5  \n",
    "Task: find frames in a {frame}-frame video where “{activity}” occurs.  \n",
    "Output exactly “n,n,…”—no words, quotes or punctuation beyond commas.''',\n",
    "    \n",
    "'''Demo: 4 frames, “jump” → 2,4  \n",
    "Instruction: From a {frame}-frame video, list all frames showing “{activity}”.  \n",
    "Answer format: comma-separated numbers only. Do NOT add explanations.''',\n",
    "    \n",
    "'''Example: 6 frames, “wave” → 1,6  \n",
    "You are given a video of {frame} frames and the activity \"{activity}\".  \n",
    "Strictly return only the frame numbers separated by commas (e.g. 1,5,10).''',\n",
    "    \n",
    "'''Example: Video of 5 frames, activity “clap”: → 1,3\n",
    "\n",
    "You are given a video of {frame} frames.  \n",
    "Task: identify all frames showing “{activity}”.  \n",
    "Strictly output exactly a comma-separated list of frame numbers (e.g. 1,5,10).  \n",
    "Do NOT include any other words, punctuation or explanation.''',\n",
    "]\n",
    "\n",
    "prompt_templates = new_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ceeb62a-0a53-405c-9d95-e4c4e23acaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2, 3, 4, 5\n",
      "[[112, 128]]\n",
      "The video is about a man who is talking to the camera.\n",
      "[[112, 128]]\n",
      "1, 2, 3, 4\n",
      "[[112, 128]]\n",
      "1,2,3,4,5,6,7,8,9,10\n",
      "[[112, 128]]\n",
      "1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20\n",
      "[[112, 128]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    preds = predict(val_jsonl[10], prompt_template=i+1)\n",
    "    print(preds[0])\n",
    "    print(val_jsonl[10]['relevant_windows'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345c493c-9812-4914-83ce-f015266808e9",
   "metadata": {},
   "source": [
    "## label boxes prompt from chatgpt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa76dd82-bfde-451f-877b-c7dc7910a2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_templates = [\n",
    "'''Example: 20-frame video, “man sitting down” → 00110  \n",
    "You have a {frame}-frame video.  \n",
    "Task: output a binary string of length {frame} where each “1” marks a frame with “{activity}” and “0” marks all others.  \n",
    "Do NOT add spaces, commas, or any extra text.''',\n",
    "\n",
    "'''Demo: 7 frames, “man sitting down” → 0001001  \n",
    "Instruction: For a {frame}-frame video, produce exactly {frame} digits (0/1) indicating frames with “{activity}”.  \n",
    "No separators or explanations—only the digit sequence.''',\n",
    "    \n",
    "'''Example: frames=20, activity=“man sitting down”, answer=12222111111111111221  \n",
    "Question: You’re given {frame} frames video and the activity “{activity}”.  \n",
    "Output exactly {frame} characters of 1 or 2, with 2 for frames containing the activity.  \n",
    "No quotes, no text, no line breaks.''',\n",
    "\n",
    "'''Demo: frames=20, activity=“man sitting down”, answer=12222111111111111221  \n",
    "Question: You have a video of {frame} frames video. Mark frames showing “{activity}” with “2” and others with “1”.  \n",
    "Return only the continuous 0/1 string of length {frame}.'''\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a79b2635-d476-40bd-bf60-4dd50766b9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00110\n",
      "[[112, 128]]\n",
      "0001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001001\n",
      "[[112, 128]]\n",
      "12222111111111111221\n",
      "[[112, 128]]\n",
      "11111111111111111221\n",
      "[[112, 128]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    preds = predict(val_jsonl[10], prompt_template=i+1)\n",
    "    print(preds[0])\n",
    "    print(val_jsonl[10]['relevant_windows'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340e9aad-8429-4c20-9da5-fa14bba04ef3",
   "metadata": {},
   "source": [
    "## better prompt version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3677fe93-7578-4d63-9dc4-16ca8ec2a7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(qvh, num_frames=20, prompt_template=1):\n",
    "    video_file = f\"videos/{qvh['vid']}.mp4\"\n",
    "    \n",
    "    vframes, _, info = torchvision.io.read_video(\n",
    "        filename=video_file, pts_unit=\"sec\", output_format=\"TCHW\"\n",
    "    )\n",
    "    fps = float(info['video_fps'])\n",
    "    total_frames = len(vframes)\n",
    "    images, timestamps = sample_frames(vframes, num_frames, fps)\n",
    "\n",
    "    prompt = \"\"\n",
    "    prompt = prompt + \"<image>\\n\"\n",
    "    # prompt = prompt + \"What's the main gist of the video ?\"\n",
    "    # prompt = prompt + \"Please describe the primary object or subject in the video, capturing their attributes, actions, positions, and movements.\"\n",
    "    demo = '1'*(num_frames//2) + '2'*(num_frames - num_frames//2)\n",
    "    prompt = prompt + prompt_templates[prompt_template-1].format(frame=num_frames, activity=qvh['query'], demo=demo)\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    generated = generate(messages, images)\n",
    "    return generated, timestamps\n",
    "\n",
    "prompt_templates = [\n",
    "'''Demo: {frame}-frame video, activity \"{activity}\" → {demo}\n",
    "Hey, there’s a {frame}-frame clip and I’m looking for moments of \"{activity}\". \n",
    "Mark each frame: use \"2\" if the activity shows up, \"1\" if not.\n",
    "Send back exactly {frame} digits, no quotes or extra words.''',\n",
    "'''\n",
    "Demo: {frame}-frame video, activity \"{activity}\" → {demo}\n",
    "Hey, there’s a {frame}-frame clip and I’m looking for moments of \"{activity}\". \n",
    "Mark each frame: use \"2\" if the activity shows up, \"1\" if not.\n",
    "Send back exactly {frame} digits, no quotes or extra words.\n",
    "\n",
    "Q: Which frames show “{activity}”?\n",
    "A:\"''',\n",
    "    \n",
    "'''Me: Here’s a {frame}-frame clip and I want to spot “{activity}”. \n",
    "Provide your answers like this: frames → {demo}\n",
    "You:''',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "734121f3-1263-434f-90f7-af527fea5265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[[112, 128]]\n",
      "The frames that show \"Old man speaks next to plaques\" are:\n",
      "[[112, 128]]\n",
      "The old man speaks next to plaques.\n",
      "[[112, 128]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    preds = predict(val_jsonl[10], prompt_template=i+1)\n",
    "    print(preds[0])\n",
    "    print(val_jsonl[10]['relevant_windows'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9049d7e9-cb18-45d4-859d-707dc5425def",
   "metadata": {},
   "source": [
    "# xgen-mm interleaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b87b9f73-a42b-425e-a1b8-701b77010e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qid': 2988,\n",
       " 'query': 'mum is talking with girls and putting shoes on girl.',\n",
       " 'duration': 150,\n",
       " 'vid': 'H3fhZxUC5M8_360.0_510.0',\n",
       " 'relevant_clip_ids': [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74],\n",
       " 'saliency_scores': [[2, 4, 3],\n",
       "  [4, 4, 4],\n",
       "  [4, 4, 3],\n",
       "  [3, 4, 3],\n",
       "  [3, 4, 3],\n",
       "  [2, 4, 3],\n",
       "  [2, 4, 3],\n",
       "  [2, 4, 3],\n",
       "  [2, 4, 3],\n",
       "  [2, 4, 4],\n",
       "  [2, 4, 4],\n",
       "  [2, 4, 3],\n",
       "  [2, 4, 3]],\n",
       " 'relevant_windows': [[124, 150]]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import stuff\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.io\n",
    "from huggingface_hub import hf_hub_download\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "from transformers import AutoModelForVision2Seq, AutoTokenizer, AutoImageProcessor, StoppingCriteria\n",
    "\n",
    "# load small set (250 questions instead of 1.5k)\n",
    "\n",
    "with open('highlight_val_small.json', 'r') as f_in:\n",
    "    val_jsonl = json.load(f_in)\n",
    "\n",
    "val_jsonl[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ddbc6f-1cdb-4a86-9ae4-f2f57f0b0065",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"Salesforce/xgen-mm-phi3-mini-instruct-interleave-r-v1.5\"\n",
    "model = AutoModelForVision2Seq.from_pretrained(model_name_or_path, trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True, use_fast=False, legacy=False) \n",
    "image_processor = AutoImageProcessor.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "tokenizer = model.update_special_tokens(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6f896a7-964c-4777-9455-de6365ab6e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda')\n",
    "model.eval()\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.eos_token = '<|end|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11003014-8b92-4753-be52-590bd0ac3d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import PIL\n",
    "import textwrap\n",
    "import IPython.display as display\n",
    "from IPython.display import Image\n",
    "\n",
    "def apply_prompt_template(prompt):\n",
    "    s = (\n",
    "                '<|system|>\\nA chat between a curious user and an artificial intelligence assistant. '\n",
    "                \"Given an activity, the assistant analyzes the images and outputs which image has high probability of containing the activity. \\\n",
    "                Then, the assistant outputs a 15 character binary mask with '1' as low probability and '2' as high probability. \\\n",
    "                Example: 111222211111111<|end|>\\n\"\n",
    "                f'<|user|>\\n{prompt}<|end|>\\n<|assistant|>\\n'\n",
    "            )\n",
    "    return s \n",
    "\n",
    "prompt_templates = [\n",
    "'''You are given {frame} images that are sampled from a video starting from image 1, image 2, and until image {frame}.\n",
    "Your task is to identify which frames are likely to show {activity}.\n",
    "Output exactly {frame} characters of 1 or 2, with 2 for frames containing the activity and 1 not. \n",
    "No quotes, no text, no line breaks.\n",
    "Example: If you think the first and second frame contains the activity, answer 22111''',\n",
    "\n",
    "'''Example: frames={frame}, activity=\"man sitting down in Bus\", answer={demo}\n",
    "You are given {frame} images sampled from a video in order from image 1 to image {frame+1}.\n",
    "Identify the frames where {activity} is happening.\n",
    "Output exactly {frame} characters using 1 or 2: write 2 if the activity appears, and 1 if it does not.\n",
    "No quotes, no extra text, no line breaks.''',\n",
    "\n",
    "'''Example: frames={frame}, activity=\"man sitting down in Bus\", answer={demo}\n",
    "You are provided with {frame} sequential images taken from a video, numbered from image 1 to image {frame+1}.\n",
    "Decide which frames show {activity}.\n",
    "Write a sequence of {frame} characters: 2 for frames showing the activity, and 1 otherwise.\n",
    "No quotes, no extra words, no line breaks.''',\n",
    "\n",
    "'''Example: frames={frame}, activity=\"man sitting down in Bus\", answer={demo}\n",
    "You are shown {frame} frames sampled from a video, starting from image 1 to image {frame+1}.\n",
    "Your task is to detect {activity} across the frames.\n",
    "Output exactly {frame} characters, marking 2 if the activity is present, and 1 if not.\n",
    "No quotes, no explanations, no line breaks.''',\n",
    "'''\n",
    "Activity: {activity} Which images have high probability of containing the activity? Output exactly {frame} characters using 1 or 2: write 2 if the activity appears, and 1 if it does not.'''\n",
    "]\n",
    "\n",
    "def make_prompt(frame, activity, prompt_idx, timestamps):\n",
    "    demo = '1'*(frame//2) + '2'*(frame - frame//2)\n",
    "    prompt = (\"<image>\"*frame) + '\\n' + prompt_templates[prompt_idx-1].format(activity=activity, frame=frame, demo=demo) \n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d3f8e52-702b-4185-8324-5cca491ba7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_frames(vframes, num_frames, fps):\n",
    "    frame_indice = np.linspace(int(num_frames/2), len(vframes) - int(num_frames/2), num_frames, dtype=int)\n",
    "    video = vframes[frame_indice]\n",
    "    video_list = []\n",
    "    for i in range(len(video)):\n",
    "        video_list.append(torchvision.transforms.functional.to_pil_image(video[i]))\n",
    "    timestamps = (frame_indice / fps).tolist() \n",
    "    return video_list, list(map(round, timestamps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1bdeb28-2cd1-4882-ba0b-f85f35eb383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_interleave(qvh, num_frames=15, template_idx=1):\n",
    "    video_file = f\"videos/{qvh['vid']}.mp4\"\n",
    "    vframes, _, info = torchvision.io.read_video(\n",
    "        filename=video_file, pts_unit=\"sec\", output_format=\"TCHW\"\n",
    "    )\n",
    "    fps = float(info['video_fps'])\n",
    "    total_frames = len(vframes)\n",
    "    images, timestamps = sample_frames(vframes, num_frames, fps)\n",
    "    image_list = []\n",
    "    for index, img in enumerate(images):\n",
    "        # display.display(img)\n",
    "        image_list.append(image_processor([img], image_aspect_ratio='anyres')['pixel_values'].cuda())\n",
    "    image_sizes = [image.size for image in images]\n",
    "    inputs = {\n",
    "        \"pixel_values\": [image_list]\n",
    "    }\n",
    "    # prompt = apply_prompt_template(\"What happens in these images chronologically?\" + (\"<image>\"*num_frames))\n",
    "    prompt = apply_prompt_template(make_prompt(num_frames, qvh['query'], template_idx, timestamps))\n",
    "    language_inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "    inputs.update(language_inputs)\n",
    "    \n",
    "    for name, value in inputs.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            inputs[name] = value.cuda()\n",
    "    generated_text = model.generate(**inputs, image_size=[image_sizes],\n",
    "                                    pad_token_id=tokenizer.pad_token_id,\n",
    "                                    eos_token_id=tokenizer.eos_token_id,\n",
    "                                    temperature=0.05,\n",
    "                                    do_sample=False, max_new_tokens=1024, top_p=None, num_beams=1,\n",
    "                                    )\n",
    "    prediction = tokenizer.decode(generated_text[0], skip_special_tokens=True).split(\"<|end|>\")[0]\n",
    "    print(\"User: \", prompt)\n",
    "    print(\"Assistant: \", textwrap.fill(prediction, width=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c00d0e4-2f38-4139-9c6a-64755710bef9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  <|system|>\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.<|end|>\n",
      "<|user|>\n",
      "What happens in these images chronologically?<image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><|end|>\n",
      "<|assistant|>\n",
      "\n",
      "Assistant:  a group of children playing in a room, with a woman standing nearby. The children are wearing\n",
      "colorful clothing, and there are toys and furniture in the room. The woman is interacting with the\n",
      "children, possibly playing with them or supervising their activities. The children are engaged in\n",
      "various activities, such as playing with toys, sitting on the floor, and standing. The woman is\n",
      "wearing a plaid shirt and appears to be in a relaxed and comfortable position. The children are\n",
      "wearing colorful clothing, and there are toys and furniture in the room. The woman is interacting\n",
      "with the children, possibly playing with them or supervising their activities. The children are\n",
      "engaged in various activities, such as playing with toys, sitting on the floor, and standing. The\n",
      "woman is wearing a plaid shirt and appears to be in a relaxed and comfortable position.\n"
     ]
    }
   ],
   "source": [
    "predict_interleave(val_jsonl[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f758c702-5a37-4dba-acf4-cee4ff2d8cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  <|system|>\n",
      "A chat between a curious user and an artificial intelligence assistant. Given an activity, the assistant analyzes the images and outputs which image has high probability of containing the activity.                 Then, the assistant outputs a 15 character binary mask with '1' as low probability and '2' as high probability.                 Example: 111222211111111<|end|>\n",
      "<|user|>\n",
      "<image><image><image><image><image><image><image><image><image><image><image><image><image><image><image>\n",
      "\n",
      "Activity: mum is talking with girls and putting shoes on girl. Which images have high probability of containing the activity? Output exactly 15 characters using 1 or 2: write 2 if the activity appears, and 1 if it does not.<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "Assistant:  1\n"
     ]
    }
   ],
   "source": [
    "predict_interleave(val_jsonl[0], template_idx=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
