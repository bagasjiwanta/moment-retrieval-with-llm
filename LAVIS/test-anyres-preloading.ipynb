{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04e6576d",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06624a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-13 18:58:35,877] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = \"/workspace/.cache/huggingface\"\n",
    "from dataclasses import dataclass\n",
    "from open_flamingo.src.factory import create_model_and_transforms\n",
    "from open_flamingo.train.sft_data_utils import LazySupervisedDataset\n",
    "from open_flamingo.train.train_utils import random_seed\n",
    "import torch\n",
    "from open_flamingo.train.sft_data_utils import DataCollatorForSupervisedDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b60dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class QvhDataArgs:\n",
    "    image_aspect_ratio = \"anyres\"\n",
    "    conv_template_name = \"phi_3\"\n",
    "    anyres_grids = [(1, 2), (2, 1), (2, 2), (3, 1), (1, 3)]\n",
    "    data_config = {\n",
    "        \"qvhighlights\": {\n",
    "            \"train\": {\n",
    "                \"annotations\": {\n",
    "                    \"../datasets/qvhighlights-sample/annotations/processed/train.json\": 3,\n",
    "                },\n",
    "                \"videos\": \"../datasets/qvhighlights/videos/processed\",\n",
    "            },\n",
    "            \"val\": {\n",
    "                \"annotations\": {\n",
    "                    \"../datasets/qvhighlights-sample/annotations/processed/train.json\": 3,\n",
    "                },\n",
    "                \"videos\": \"../datasets/qvhighlights/videos/processed\",\n",
    "            },\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b91e71c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3f5b0a574f41b0bb85e458316ae7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgenmm_v1 model initialized with 3,931,031,619 trainable parameters\n",
      "==========Trainable Parameters\n",
      "Vision encoder: 0 trainable parameters\n",
      "Vision tokenizer: 109,901,568 trainable parameters\n",
      "Language model: 3,821,130,051 trainable parameters\n",
      "==========Total Parameters\n",
      "Vision encoder: 428,225,600 parameters\n",
      "Vision tokenizer: 109,901,568 parameters\n",
      "Language model: 3,821,130,051 parameters\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "model, image_processor, text_tokenizer = create_model_and_transforms(\n",
    "    vision_encoder_path=\"google/siglip-so400m-patch14-384\",\n",
    "    lang_model_path=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    anyres_grids=[(1, 2), (2, 1), (2, 2), (3, 1), (1, 3)],\n",
    "    tokenizer_path=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    model_family=\"xgenmm_v1\",\n",
    "    pretrained_vision_tokenizer=None,\n",
    "    use_local_files=False,\n",
    "    verbose=True,\n",
    "    use_flash_attention_2=True,\n",
    "    image_aspect_ratio=\"anyres\",\n",
    "    num_vision_tokens=128,\n",
    "    anyres_patch_sampling=True,\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "random_seed(42)\n",
    "# model = model.to(\"cuda\", dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66db06ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizer(name_or_path='microsoft/Phi-3-mini-4k-instruct', vocab_size=32000, model_max_length=4096, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|endoftext|>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<image>', '<image placeholder>', '<|endofchunk|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t32000: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32001: AddedToken(\"<|assistant|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32002: AddedToken(\"<|placeholder1|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32003: AddedToken(\"<|placeholder2|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32004: AddedToken(\"<|placeholder3|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32005: AddedToken(\"<|placeholder4|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32006: AddedToken(\"<|system|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32007: AddedToken(\"<|end|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32008: AddedToken(\"<|placeholder5|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32009: AddedToken(\"<|placeholder6|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32010: AddedToken(\"<|user|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32011: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32012: AddedToken(\"<image>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32013: AddedToken(\"<image placeholder>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32014: AddedToken(\"<|endofchunk|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0767e52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"qvhighlights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9222ca89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10d1adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args = QvhDataArgs() if dataset_name == \"qvhighlights\" else \"charades-sta\"\n",
    "train_dataset = LazySupervisedDataset(\n",
    "    tokenizer=text_tokenizer,\n",
    "    image_processor=image_processor,\n",
    "    split=\"train\",\n",
    "    data_args=data_args,\n",
    "    data_config=data_args.data_config,\n",
    ")\n",
    "\n",
    "val_dataset = LazySupervisedDataset(\n",
    "    tokenizer=text_tokenizer,\n",
    "    image_processor=image_processor,\n",
    "    split=\"val\",\n",
    "    data_args=data_args,\n",
    "    data_config=data_args.data_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be357356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"<|system|> A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. <|end|> <|user|> <image> \\nDescribe the woman in the image <|end|> <|assistant|>\"],\n",
       " [\"<|system|> A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. <|end|> <|user|> <image> \\nWhat can be seen in the image? <|end|> <|assistant|>\"],\n",
       " [\"<|system|> A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. <|end|> <|user|> <image> \\nWhat happens in the image? <|end|> <|assistant|>\"])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(text_tokenizer.batch_decode(val_dataset[0]['input_ids'].unsqueeze(0)),\n",
    "text_tokenizer.batch_decode(val_dataset[1]['input_ids'].unsqueeze(0)),\n",
    "text_tokenizer.batch_decode(val_dataset[2]['input_ids'].unsqueeze(0))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bcc7c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'val'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9a35b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'labels', 'qid', 'image', 'image_size'])\n",
      " k='input_ids', type(v)=<class 'torch.Tensor'>\n",
      " k='labels', type(v)=<class 'torch.Tensor'>\n",
      " k='qid', type(v)=<class 'int'>\n",
      " k='image', type(v)=<class 'list'>\n",
      " k='image_size', type(v)=<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "train_sample = train_dataset[0]\n",
    "print(train_sample.keys())\n",
    "for k, v in train_sample.items():\n",
    "    print(f\" {k=}, {type(v)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e307424d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([29871, 32006, 29909, 13563,  1546,   263, 12758,  1404,   322,   385,\n",
       "        23116, 21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173,\n",
       "        29892,   322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,\n",
       "         5155, 29889, 32007, 32010, 32012,    13,  4002, 29581,   278,  6114,\n",
       "          297,   278,  1967, 32007, 32001, 29956,  2480,   297,  2174,   333,\n",
       "          528,  2728,   269,  1169,  1603,   491,   263,  2654,   274,  3222,\n",
       "        29889, 32007])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c81989d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100, 29956,  2480,   297,  2174,   333,\n",
       "          528,  2728,   269,  1169,  1603,   491,   263,  2654,   274,  3222,\n",
       "        29889, 32007])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e793355d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'labels', 'duration', 'vid', 'qid', 'image', 'image_size'])\n",
      " k='input_ids', type(v)=<class 'torch.Tensor'>\n",
      " k='labels', type(v)=<class 'str'>\n",
      " k='duration', type(v)=<class 'int'>\n",
      " k='vid', type(v)=<class 'str'>\n",
      " k='qid', type(v)=<class 'int'>\n",
      " k='image', type(v)=<class 'list'>\n",
      " k='image_size', type(v)=<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "val_sample = val_dataset[2]\n",
    "print(val_sample.keys())\n",
    "for k, v in val_sample.items():\n",
    "    print(f\" {k=}, {type(v)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "644c54fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It looks like two person is cooking with facetime'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_sample['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f310aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collator = DataCollatorForSupervisedDataset(\n",
    "    tokenizer=text_tokenizer, image_aspect_ratio=\"anyres\", split=\"train\"\n",
    ")\n",
    "\n",
    "val_collator = DataCollatorForSupervisedDataset(\n",
    "    tokenizer=text_tokenizer, image_aspect_ratio=\"anyres\", split=\"val\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92104313",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    shuffle=True,\n",
    "    collate_fn=train_collator,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=2,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    shuffle=True,\n",
    "    collate_fn=val_collator,\n",
    "    persistent_workers=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc5785",
   "metadata": {},
   "source": [
    "# Test Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad5c962c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val\n",
      "val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val\n",
      "val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "461f72bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'labels', 'attention_mask', 'metadata', 'image_size', 'images'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0756deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[29871, 32006, 29909, 13563,  1546,   263, 12758,  1404,   322,   385,\n",
       "          23116, 21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173,\n",
       "          29892,   322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,\n",
       "           5155, 29889, 32007, 32010, 32012,    13,  5618,   508,   367,  3595,\n",
       "            297,   278,  1967, 29973, 32007, 32001],\n",
       "         [32011, 32011, 29871, 32006, 29909, 13563,  1546,   263, 12758,  1404,\n",
       "            322,   385, 23116, 21082, 20255, 29889,   450, 20255,  4076,  8444,\n",
       "          29892, 13173, 29892,   322,  1248,   568,  6089,   304,   278,  1404,\n",
       "          29915, 29879,  5155, 29889, 32007, 32010, 32012,    13,  5618,  5930,\n",
       "            297,   278,  1967, 29973, 32007, 32001]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    batch['input_ids'],\n",
    "    batch['attention_mask']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a5f366f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Words that says 'Even though I still live close by,'\",\n",
       " 'It looks like two person is cooking with facetime']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0541adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<|system|> A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. <|end|> <|user|> <image> \\nWhat can be seen in the image? <|end|> <|assistant|>\",\n",
       " \"<pad><pad>  <|system|> A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. <|end|> <|user|> <image> \\nWhat happens in the image? <|end|> <|assistant|>\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenizer.batch_decode(batch['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "427a136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['input_ids'] = batch['input_ids'].to('cuda')\n",
    "batch['attention_mask'] = batch['attention_mask'].to('cuda')\n",
    "batch['images'][0][0] = batch['images'][0][0].to('cuda')\n",
    "batch['images'][1][0] = batch['images'][1][0].to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c90a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2863a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e29fe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2ee7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f8084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['input_ids'].shape, batch['labels'].shape, batch['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710bb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['image_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd24bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch['images']), len(batch['images'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7365f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['images'][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54594afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer.batch_decode(batch['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de699a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c196ceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d94a532",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_labels = torch.where(batch['labels'] == -100, torch.ones_like(batch['labels']) * 32011, batch['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147fbee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer.batch_decode(\n",
    "    pad_labels, skip_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d721b67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12984/4223646711.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"../base_model_weight/xgen-mm-phi3-mini-base-r-v1.5.pt\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_special_tokens': {'media_token': '<image>',\n",
       "  'image_placeholder_token': '<image placeholder>',\n",
       "  'end_of_trunk_token': '<|endofchunk|>'},\n",
       " 'training': True,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_hooks_always_called': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict([('vision_encoder',\n",
       "               SiglipVisionTransformer(\n",
       "                 (embeddings): SiglipVisionEmbeddings(\n",
       "                   (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "                   (position_embedding): Embedding(729, 1152)\n",
       "                 )\n",
       "                 (encoder): SiglipEncoder(\n",
       "                   (layers): ModuleList(\n",
       "                     (0-26): 27 x SiglipEncoderLayer(\n",
       "                       (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "                       (self_attn): SiglipAttention(\n",
       "                         (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                         (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                         (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                         (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                       )\n",
       "                       (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "                       (mlp): SiglipMLP(\n",
       "                         (activation_fn): PytorchGELUTanh()\n",
       "                         (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                         (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "                       )\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "                 (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "                 (head): SiglipMultiheadAttentionPoolingHead(\n",
       "                   (attention): MultiheadAttention(\n",
       "                     (out_proj): NonDynamicallyQuantizableLinear(in_features=1152, out_features=1152, bias=True)\n",
       "                   )\n",
       "                   (layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "                   (mlp): SiglipMLP(\n",
       "                     (activation_fn): PytorchGELUTanh()\n",
       "                     (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                     (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "                   )\n",
       "                 )\n",
       "               )),\n",
       "              ('vision_tokenizer',\n",
       "               PerceiverResampler(\n",
       "                 (projection): Linear(in_features=1152, out_features=3072, bias=True)\n",
       "                 (layers): ModuleList(\n",
       "                   (0-5): 6 x ModuleList(\n",
       "                     (0): PerceiverAttention(\n",
       "                       (norm_media): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "                       (norm_latents): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "                       (to_q): Linear(in_features=1152, out_features=1536, bias=False)\n",
       "                       (to_kv): Linear(in_features=1152, out_features=3072, bias=False)\n",
       "                       (to_out): Linear(in_features=1536, out_features=1152, bias=False)\n",
       "                     )\n",
       "                     (1): Sequential(\n",
       "                       (0): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "                       (1): Linear(in_features=1152, out_features=4608, bias=False)\n",
       "                       (2): GELU(approximate='none')\n",
       "                       (3): Linear(in_features=4608, out_features=1152, bias=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "                 (norm): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "               )),\n",
       "              ('lang_model',\n",
       "               Phi3ForCausalLM(\n",
       "                 (model): Phi3Model(\n",
       "                   (embed_tokens): DecoupledEmbedding(\n",
       "                     num_original_embeddings=32012, num_additional_embeddings=3, embedding_dim=3072, partially_freeze=False\n",
       "                     (additional_embedding): Embedding(3, 3072)\n",
       "                   )\n",
       "                   (layers): ModuleList(\n",
       "                     (0-31): 32 x Phi3DecoderLayer(\n",
       "                       (self_attn): Phi3Attention(\n",
       "                         (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "                         (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "                       )\n",
       "                       (mlp): Phi3MLP(\n",
       "                         (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "                         (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "                         (activation_fn): SiLU()\n",
       "                       )\n",
       "                       (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "                       (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "                       (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                       (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "                   (rotary_emb): Phi3RotaryEmbedding()\n",
       "                 )\n",
       "                 (lm_head): DecoupledLinear(\n",
       "                   in_features=3072, out_features=32012, additional_out_features=3, bias=True, partially_freeze=False\n",
       "                   (additional_fc): Linear(in_features=3072, out_features=3, bias=True)\n",
       "                 )\n",
       "               ))]),\n",
       " 'lang_embedding_dim': 3072,\n",
       " 'lang_hidden_dim': 3072,\n",
       " 'vis_embedding_dim': 1152,\n",
       " 'num_tokens_per_vis': 128,\n",
       " 'base_img_size': 384,\n",
       " 'pad_token_id': 32011,\n",
       " 'initial_tokenizer_len': 32012,\n",
       " 'decoder_layers_attr_name': 'model.layers',\n",
       " 'image_aspect_ratio': 'anyres',\n",
       " 'anyres_patch_sampling': True,\n",
       " 'anyres_grids': [[384, 768],\n",
       "  [768, 384],\n",
       "  [768, 768],\n",
       "  [1152, 384],\n",
       "  [384, 1152]],\n",
       " 'media_token_id': 32012,\n",
       " 'image_placeholder_token_id': 32013,\n",
       " 'end_of_trunk_token_id': 32014}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(\"../base_model_weight/xgen-mm-phi3-mini-base-r-v1.5.pt\")\n",
    "model.load_state_dict(ckpt, strict=True)\n",
    "torch.cuda.empty_cache()\n",
    "model = model.to(\"cuda\", dtype=torch.bfloat16)\n",
    "vars(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c96024f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 13 19:04:57 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A5000               On  |   00000000:D1:00.0 Off |                  Off |\n",
      "| 30%   34C    P0             79W /  230W |    8610MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda5860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs_default = dict(do_sample=False, max_new_tokens=1024, top_p=None, num_beams=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_text = model.generate(\n",
    "        vision_x=batch['images'],\n",
    "        lang_x=batch['input_ids'],\n",
    "        image_size=batch['image_size'],\n",
    "        attention_mask=batch['attention_mask'],\n",
    "        **kwargs_default\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9351c6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The image presents a minimalist design with a white background and black text. The text, centered and in a sans-serif font, reads, \"Even though I still live close by.\" This design, with its clean and uncluttered appearance, effectively conveys a sense of simplicity and clarity.',\n",
       " 'The image appears to be a screenshot of a video call or a live stream. On the left side, there is a video feed showing a person cooking in a kitchen. The person seems to be frying or boiling potatoes in a skillet. On the right side, there is a smaller video feed showing a close-up of the same skillet with the potatoes. The person in the kitchen is wearing a pink sweater and has a watch on their wrist. There are also some icons and controls at the bottom of the image, suggesting that this might be a screenshot from a video conferencing application. The image contains poster (in the center), text (above the center), text (below the center)']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenizer.batch_decode(generated_text, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9318f27c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
