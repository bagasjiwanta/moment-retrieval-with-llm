[(1, 2), (2, 1), (2, 2), (3, 1), (1, 3)]
Initializing distributed training with 1 GPUs.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.15s/it]
xgenmm_v1 model initialized with 3,931,031,619 trainable parameters
==========Trainable Parameters
Vision encoder: 0 trainable parameters
Vision tokenizer: 109,901,568 trainable parameters
Language model: 3,821,130,051 trainable parameters
==========Total Parameters
Vision encoder: 428,225,600 parameters
Vision tokenizer: 109,901,568 parameters
Language model: 3,821,130,051 parameters
==========
model [(1, 2), (2, 1), (2, 2), (3, 1), (1, 3)]
Found no checkpoints for run finetune-xgenmmv1-phi3_4k_instruct-detail_23k_lora_1.
Loading checkpoint from /workspace/LAVIS/base_model_weight/xgen-mm-phi3-mini-base-r-v1.5.pt
Missing keys: []
Unexpected keys: []
Finished loading checkpoint...
['lang_model.model.layers.0.self_attn.o_proj', 'lang_model.model.layers.0.self_attn.qkv_proj', 'lang_model.model.layers.0.mlp.gate_up_proj', 'lang_model.model.layers.0.mlp.down_proj', 'lang_model.model.layers.1.self_attn.o_proj', 'lang_model.model.layers.1.self_attn.qkv_proj', 'lang_model.model.layers.1.mlp.gate_up_proj', 'lang_model.model.layers.1.mlp.down_proj', 'lang_model.model.layers.2.self_attn.o_proj', 'lang_model.model.layers.2.self_attn.qkv_proj', 'lang_model.model.layers.2.mlp.gate_up_proj', 'lang_model.model.layers.2.mlp.down_proj', 'lang_model.model.layers.3.self_attn.o_proj', 'lang_model.model.layers.3.self_attn.qkv_proj', 'lang_model.model.layers.3.mlp.gate_up_proj', 'lang_model.model.layers.3.mlp.down_proj', 'lang_model.model.layers.4.self_attn.o_proj', 'lang_model.model.layers.4.self_attn.qkv_proj', 'lang_model.model.layers.4.mlp.gate_up_proj', 'lang_model.model.layers.4.mlp.down_proj', 'lang_model.model.layers.5.self_attn.o_proj', 'lang_model.model.layers.5.self_attn.qkv_proj', 'lang_model.model.layers.5.mlp.gate_up_proj', 'lang_model.model.layers.5.mlp.down_proj', 'lang_model.model.layers.6.self_attn.o_proj', 'lang_model.model.layers.6.self_attn.qkv_proj', 'lang_model.model.layers.6.mlp.gate_up_proj', 'lang_model.model.layers.6.mlp.down_proj', 'lang_model.model.layers.7.self_attn.o_proj', 'lang_model.model.layers.7.self_attn.qkv_proj', 'lang_model.model.layers.7.mlp.gate_up_proj', 'lang_model.model.layers.7.mlp.down_proj', 'lang_model.model.layers.8.self_attn.o_proj', 'lang_model.model.layers.8.self_attn.qkv_proj', 'lang_model.model.layers.8.mlp.gate_up_proj', 'lang_model.model.layers.8.mlp.down_proj', 'lang_model.model.layers.9.self_attn.o_proj', 'lang_model.model.layers.9.self_attn.qkv_proj', 'lang_model.model.layers.9.mlp.gate_up_proj', 'lang_model.model.layers.9.mlp.down_proj', 'lang_model.model.layers.10.self_attn.o_proj', 'lang_model.model.layers.10.self_attn.qkv_proj', 'lang_model.model.layers.10.mlp.gate_up_proj', 'lang_model.model.layers.10.mlp.down_proj', 'lang_model.model.layers.11.self_attn.o_proj', 'lang_model.model.layers.11.self_attn.qkv_proj', 'lang_model.model.layers.11.mlp.gate_up_proj', 'lang_model.model.layers.11.mlp.down_proj', 'lang_model.model.layers.12.self_attn.o_proj', 'lang_model.model.layers.12.self_attn.qkv_proj', 'lang_model.model.layers.12.mlp.gate_up_proj', 'lang_model.model.layers.12.mlp.down_proj', 'lang_model.model.layers.13.self_attn.o_proj', 'lang_model.model.layers.13.self_attn.qkv_proj', 'lang_model.model.layers.13.mlp.gate_up_proj', 'lang_model.model.layers.13.mlp.down_proj', 'lang_model.model.layers.14.self_attn.o_proj', 'lang_model.model.layers.14.self_attn.qkv_proj', 'lang_model.model.layers.14.mlp.gate_up_proj', 'lang_model.model.layers.14.mlp.down_proj', 'lang_model.model.layers.15.self_attn.o_proj', 'lang_model.model.layers.15.self_attn.qkv_proj', 'lang_model.model.layers.15.mlp.gate_up_proj', 'lang_model.model.layers.15.mlp.down_proj', 'lang_model.model.layers.16.self_attn.o_proj', 'lang_model.model.layers.16.self_attn.qkv_proj', 'lang_model.model.layers.16.mlp.gate_up_proj', 'lang_model.model.layers.16.mlp.down_proj', 'lang_model.model.layers.17.self_attn.o_proj', 'lang_model.model.layers.17.self_attn.qkv_proj', 'lang_model.model.layers.17.mlp.gate_up_proj', 'lang_model.model.layers.17.mlp.down_proj', 'lang_model.model.layers.18.self_attn.o_proj', 'lang_model.model.layers.18.self_attn.qkv_proj', 'lang_model.model.layers.18.mlp.gate_up_proj', 'lang_model.model.layers.18.mlp.down_proj', 'lang_model.model.layers.19.self_attn.o_proj', 'lang_model.model.layers.19.self_attn.qkv_proj', 'lang_model.model.layers.19.mlp.gate_up_proj', 'lang_model.model.layers.19.mlp.down_proj', 'lang_model.model.layers.20.self_attn.o_proj', 'lang_model.model.layers.20.self_attn.qkv_proj', 'lang_model.model.layers.20.mlp.gate_up_proj', 'lang_model.model.layers.20.mlp.down_proj', 'lang_model.model.layers.21.self_attn.o_proj', 'lang_model.model.layers.21.self_attn.qkv_proj', 'lang_model.model.layers.21.mlp.gate_up_proj', 'lang_model.model.layers.21.mlp.down_proj', 'lang_model.model.layers.22.self_attn.o_proj', 'lang_model.model.layers.22.self_attn.qkv_proj', 'lang_model.model.layers.22.mlp.gate_up_proj', 'lang_model.model.layers.22.mlp.down_proj', 'lang_model.model.layers.23.self_attn.o_proj', 'lang_model.model.layers.23.self_attn.qkv_proj', 'lang_model.model.layers.23.mlp.gate_up_proj', 'lang_model.model.layers.23.mlp.down_proj', 'lang_model.model.layers.24.self_attn.o_proj', 'lang_model.model.layers.24.self_attn.qkv_proj', 'lang_model.model.layers.24.mlp.gate_up_proj', 'lang_model.model.layers.24.mlp.down_proj', 'lang_model.model.layers.25.self_attn.o_proj', 'lang_model.model.layers.25.self_attn.qkv_proj', 'lang_model.model.layers.25.mlp.gate_up_proj', 'lang_model.model.layers.25.mlp.down_proj', 'lang_model.model.layers.26.self_attn.o_proj', 'lang_model.model.layers.26.self_attn.qkv_proj', 'lang_model.model.layers.26.mlp.gate_up_proj', 'lang_model.model.layers.26.mlp.down_proj', 'lang_model.model.layers.27.self_attn.o_proj', 'lang_model.model.layers.27.self_attn.qkv_proj', 'lang_model.model.layers.27.mlp.gate_up_proj', 'lang_model.model.layers.27.mlp.down_proj', 'lang_model.model.layers.28.self_attn.o_proj', 'lang_model.model.layers.28.self_attn.qkv_proj', 'lang_model.model.layers.28.mlp.gate_up_proj', 'lang_model.model.layers.28.mlp.down_proj', 'lang_model.model.layers.29.self_attn.o_proj', 'lang_model.model.layers.29.self_attn.qkv_proj', 'lang_model.model.layers.29.mlp.gate_up_proj', 'lang_model.model.layers.29.mlp.down_proj', 'lang_model.model.layers.30.self_attn.o_proj', 'lang_model.model.layers.30.self_attn.qkv_proj', 'lang_model.model.layers.30.mlp.gate_up_proj', 'lang_model.model.layers.30.mlp.down_proj', 'lang_model.model.layers.31.self_attn.o_proj', 'lang_model.model.layers.31.self_attn.qkv_proj', 'lang_model.model.layers.31.mlp.gate_up_proj', 'lang_model.model.layers.31.mlp.down_proj', 'lang_model.lm_head.additional_fc']
trainable params: 25,215,024 || all params: 4,384,472,243 || trainable%: 0.5751
================== Data mixture config ===================
{'data_path': {'/workspace/detail_23k.json': 2000}}
==========================================================
dataloader [[384, 768], [768, 384], [768, 768], [1152, 384], [384, 1152]]
Total training steps: 250
DistributedDataParallel(
  (module): PeftModel(
    (base_model): LoraModel(
      (model): XGenMMPerceiver(
        (vision_encoder): SiglipVisionTransformer(
          (embeddings): SiglipVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)
            (position_embedding): Embedding(729, 1152)
          )
          (encoder): SiglipEncoder(
            (layers): ModuleList(
              (0-26): 27 x SiglipEncoderLayer(
                (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
                (self_attn): SiglipAttention(
                  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)
                  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)
                  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)
                  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)
                )
                (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
                (mlp): SiglipMLP(
                  (activation_fn): PytorchGELUTanh()
                  (fc1): Linear(in_features=1152, out_features=4304, bias=True)
                  (fc2): Linear(in_features=4304, out_features=1152, bias=True)
                )
              )
            )
          )
          (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
          (head): SiglipMultiheadAttentionPoolingHead(
            (attention): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1152, out_features=1152, bias=True)
            )
            (layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
            (mlp): SiglipMLP(
              (activation_fn): PytorchGELUTanh()
              (fc1): Linear(in_features=1152, out_features=4304, bias=True)
              (fc2): Linear(in_features=4304, out_features=1152, bias=True)
            )
          )
        )
        (vision_tokenizer): CheckpointWrapper(
          (_checkpoint_wrapped_module): PerceiverResampler(
            (projection): Linear(in_features=1152, out_features=3072, bias=True)
            (layers): ModuleList(
              (0-5): 6 x ModuleList(
                (0): PerceiverAttention(
                  (norm_media): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)
                  (norm_latents): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)
                  (to_q): Linear(in_features=1152, out_features=1536, bias=False)
                  (to_kv): Linear(in_features=1152, out_features=3072, bias=False)
                  (to_out): Linear(in_features=1536, out_features=1152, bias=False)
                )
                (1): Sequential(
                  (0): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)
                  (1): Linear(in_features=1152, out_features=4608, bias=False)
                  (2): GELU(approximate='none')
                  (3): Linear(in_features=4608, out_features=1152, bias=False)
                )
              )
            )
            (norm): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)
          )
        )
        (lang_model): Phi3ForCausalLM(
          (model): Phi3Model(
            (embed_tokens): DecoupledEmbedding(
              num_original_embeddings=32012, num_additional_embeddings=3, embedding_dim=3072, partially_freeze=True
              (additional_embedding): Embedding(3, 3072)
            )
            (embed_dropout): Dropout(p=0.0, inplace=False)
            (layers): ModuleList(
              (0-31): 32 x CheckpointWrapper(
                (_checkpoint_wrapped_module): Phi3DecoderLayer(
                  (self_attn): Phi3Attention(
                    (o_proj): lora.Linear(
                      (base_layer): Linear(in_features=3072, out_features=3072, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.05, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): Linear(in_features=3072, out_features=16, bias=False)
                      )
                      (lora_B): ModuleDict(
                        (default): Linear(in_features=16, out_features=3072, bias=False)
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                      (lora_magnitude_vector): ModuleDict()
                    )
                    (qkv_proj): lora.Linear(
                      (base_layer): Linear(in_features=3072, out_features=9216, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.05, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): Linear(in_features=3072, out_features=16, bias=False)
                      )
                      (lora_B): ModuleDict(
                        (default): Linear(in_features=16, out_features=9216, bias=False)
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                      (lora_magnitude_vector): ModuleDict()
                    )
                    (rotary_emb): Phi3RotaryEmbedding()
                  )
                  (mlp): Phi3MLP(
                    (gate_up_proj): lora.Linear(
                      (base_layer): Linear(in_features=3072, out_features=16384, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.05, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): Linear(in_features=3072, out_features=16, bias=False)
                      )
                      (lora_B): ModuleDict(
                        (default): Linear(in_features=16, out_features=16384, bias=False)
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                      (lora_magnitude_vector): ModuleDict()
                    )
                    (down_proj): lora.Linear(
                      (base_layer): Linear(in_features=8192, out_features=3072, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.05, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): Linear(in_features=8192, out_features=16, bias=False)
                      )
                      (lora_B): ModuleDict(
                        (default): Linear(in_features=16, out_features=3072, bias=False)
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                      (lora_magnitude_vector): ModuleDict()
                    )
                    (activation_fn): SiLU()
                  )
                  (input_layernorm): Phi3RMSNorm()
                  (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  (post_attention_layernorm): Phi3RMSNorm()
                )
              )
            )
            (norm): Phi3RMSNorm()
          )
          (lm_head): DecoupledLinear(
            in_features=3072, out_features=32012, additional_out_features=3, bias=True, partially_freeze=True
            (additional_fc): lora.Linear(
              (base_layer): Linear(in_features=3072, out_features=3, bias=True)
              (lora_dropout): ModuleDict(
                (default): Dropout(p=0.05, inplace=False)
              )
              (lora_A): ModuleDict(
                (default): Linear(in_features=3072, out_features=16, bias=False)
              )
              (lora_B): ModuleDict(
                (default): Linear(in_features=16, out_features=3, bias=False)
              )
              (lora_embedding_A): ParameterDict()
              (lora_embedding_B): ParameterDict()
              (lora_magnitude_vector): ModuleDict()
            )
          )
        )
      )
    )
  )
)
Start running training on rank 0.
  0%|          | 0/2000 [00:00<?, ?it/s]Forward Call
images.shape=torch.Size([1, 1, 5, 3, 384, 384]) image_size=[[640, 480]] input_ids.shape=torch.Size([1, 220]) attention_mask.shape=torch.Size([1, 220]) labels.shape=torch.Size([1, 220])

  0%|          | 0/2000 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/workspace/LAVIS/open_flamingo/train/instruction_finetune.py", line 501, in <module>
    main()
  File "/workspace/LAVIS/open_flamingo/train/instruction_finetune.py", line 483, in main
    finetune_one_epoch(
  File "/workspace/LAVIS/open_flamingo/train/train_utils.py", line 301, in finetune_one_epoch
    loss = compute_loss_fn(
  File "/workspace/LAVIS/open_flamingo/train/losses.py", line 111, in __call__
    loss = model(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/peft/peft_model.py", line 818, in forward
    return self.get_base_model()(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/LAVIS/open_flamingo/src/xgenmm.py", line 121, in forward
    vision_features, vision_attn_masks = self._encode_vision_x_anyres(input_dict, lang_x.device)
  File "/workspace/LAVIS/open_flamingo/src/vlm.py", line 244, in _encode_vision_x_anyres
    patch_embeds = patch_embeds.view(num_patch_height, num_patch_width, height, width, -1)
RuntimeError: shape '[0, 0, 27, 27, -1]' is invalid for input of size 3359232
[2025-05-01 18:20:29,672] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 17822) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 810, in <module>
    main()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
open_flamingo/train/instruction_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-05-01_18:20:29
  host      : 86a95e6a4627
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 17822)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
