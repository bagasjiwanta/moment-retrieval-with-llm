/usr/local/lib/python3.10/dist-packages/debugpy/_vendored/force_pydevd.py:18: UserWarning: incompatible copy of pydevd already imported:
 /usr/local/lib/python3.10/dist-packages/pydevd_plugins/__init__.py
  /usr/local/lib/python3.10/dist-packages/pydevd_plugins/extensions/__init__.py
  /usr/local/lib/python3.10/dist-packages/pydevd_plugins/extensions/pydevd_plugin_omegaconf.py
  warnings.warn(msg + ':\n {}'.format('\n  '.join(_unvendored)))
[(1, 2), (2, 1), (2, 2), (3, 1), (1, 3)]
Initializing distributed training with 1 GPUs.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.76s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.71s/it]
xgenmm_v1 model initialized with 3,931,031,619 trainable parameters
==========Trainable Parameters
Vision encoder: 0 trainable parameters
Vision tokenizer: 109,901,568 trainable parameters
Language model: 3,821,130,051 trainable parameters
==========Total Parameters
Vision encoder: 428,225,600 parameters
Vision tokenizer: 109,901,568 parameters
Language model: 3,821,130,051 parameters
==========
model [(1, 2), (2, 1), (2, 2), (3, 1), (1, 3)]
Found no checkpoints for run finetune-xgenmmv1-phi3_4k_instruct-detail_23k-lora-020525-1.
Loading checkpoint from /workspace/LAVIS/base_model_weight/xgen-mm-phi3-mini-base-r-v1.5.pt
Missing keys: []
Unexpected keys: []
Finished loading checkpoint...
['vision_tokenizer.projection', 'vision_tokenizer.layers.0.0.to_q', 'vision_tokenizer.layers.0.0.to_kv', 'vision_tokenizer.layers.0.0.to_out', 'vision_tokenizer.layers.0.1.1', 'vision_tokenizer.layers.0.1.3', 'vision_tokenizer.layers.1.0.to_q', 'vision_tokenizer.layers.1.0.to_kv', 'vision_tokenizer.layers.1.0.to_out', 'vision_tokenizer.layers.1.1.1', 'vision_tokenizer.layers.1.1.3', 'vision_tokenizer.layers.2.0.to_q', 'vision_tokenizer.layers.2.0.to_kv', 'vision_tokenizer.layers.2.0.to_out', 'vision_tokenizer.layers.2.1.1', 'vision_tokenizer.layers.2.1.3', 'vision_tokenizer.layers.3.0.to_q', 'vision_tokenizer.layers.3.0.to_kv', 'vision_tokenizer.layers.3.0.to_out', 'vision_tokenizer.layers.3.1.1', 'vision_tokenizer.layers.3.1.3', 'vision_tokenizer.layers.4.0.to_q', 'vision_tokenizer.layers.4.0.to_kv', 'vision_tokenizer.layers.4.0.to_out', 'vision_tokenizer.layers.4.1.1', 'vision_tokenizer.layers.4.1.3', 'vision_tokenizer.layers.5.0.to_q', 'vision_tokenizer.layers.5.0.to_kv', 'vision_tokenizer.layers.5.0.to_out', 'vision_tokenizer.layers.5.1.1', 'vision_tokenizer.layers.5.1.3', 'lang_model.model.layers.0.self_attn.o_proj', 'lang_model.model.layers.0.self_attn.qkv_proj', 'lang_model.model.layers.0.mlp.gate_up_proj', 'lang_model.model.layers.0.mlp.down_proj', 'lang_model.model.layers.1.self_attn.o_proj', 'lang_model.model.layers.1.self_attn.qkv_proj', 'lang_model.model.layers.1.mlp.gate_up_proj', 'lang_model.model.layers.1.mlp.down_proj', 'lang_model.model.layers.2.self_attn.o_proj', 'lang_model.model.layers.2.self_attn.qkv_proj', 'lang_model.model.layers.2.mlp.gate_up_proj', 'lang_model.model.layers.2.mlp.down_proj', 'lang_model.model.layers.3.self_attn.o_proj', 'lang_model.model.layers.3.self_attn.qkv_proj', 'lang_model.model.layers.3.mlp.gate_up_proj', 'lang_model.model.layers.3.mlp.down_proj', 'lang_model.model.layers.4.self_attn.o_proj', 'lang_model.model.layers.4.self_attn.qkv_proj', 'lang_model.model.layers.4.mlp.gate_up_proj', 'lang_model.model.layers.4.mlp.down_proj', 'lang_model.model.layers.5.self_attn.o_proj', 'lang_model.model.layers.5.self_attn.qkv_proj', 'lang_model.model.layers.5.mlp.gate_up_proj', 'lang_model.model.layers.5.mlp.down_proj', 'lang_model.model.layers.6.self_attn.o_proj', 'lang_model.model.layers.6.self_attn.qkv_proj', 'lang_model.model.layers.6.mlp.gate_up_proj', 'lang_model.model.layers.6.mlp.down_proj', 'lang_model.model.layers.7.self_attn.o_proj', 'lang_model.model.layers.7.self_attn.qkv_proj', 'lang_model.model.layers.7.mlp.gate_up_proj', 'lang_model.model.layers.7.mlp.down_proj', 'lang_model.model.layers.8.self_attn.o_proj', 'lang_model.model.layers.8.self_attn.qkv_proj', 'lang_model.model.layers.8.mlp.gate_up_proj', 'lang_model.model.layers.8.mlp.down_proj', 'lang_model.model.layers.9.self_attn.o_proj', 'lang_model.model.layers.9.self_attn.qkv_proj', 'lang_model.model.layers.9.mlp.gate_up_proj', 'lang_model.model.layers.9.mlp.down_proj', 'lang_model.model.layers.10.self_attn.o_proj', 'lang_model.model.layers.10.self_attn.qkv_proj', 'lang_model.model.layers.10.mlp.gate_up_proj', 'lang_model.model.layers.10.mlp.down_proj', 'lang_model.model.layers.11.self_attn.o_proj', 'lang_model.model.layers.11.self_attn.qkv_proj', 'lang_model.model.layers.11.mlp.gate_up_proj', 'lang_model.model.layers.11.mlp.down_proj', 'lang_model.model.layers.12.self_attn.o_proj', 'lang_model.model.layers.12.self_attn.qkv_proj', 'lang_model.model.layers.12.mlp.gate_up_proj', 'lang_model.model.layers.12.mlp.down_proj', 'lang_model.model.layers.13.self_attn.o_proj', 'lang_model.model.layers.13.self_attn.qkv_proj', 'lang_model.model.layers.13.mlp.gate_up_proj', 'lang_model.model.layers.13.mlp.down_proj', 'lang_model.model.layers.14.self_attn.o_proj', 'lang_model.model.layers.14.self_attn.qkv_proj', 'lang_model.model.layers.14.mlp.gate_up_proj', 'lang_model.model.layers.14.mlp.down_proj', 'lang_model.model.layers.15.self_attn.o_proj', 'lang_model.model.layers.15.self_attn.qkv_proj', 'lang_model.model.layers.15.mlp.gate_up_proj', 'lang_model.model.layers.15.mlp.down_proj', 'lang_model.model.layers.16.self_attn.o_proj', 'lang_model.model.layers.16.self_attn.qkv_proj', 'lang_model.model.layers.16.mlp.gate_up_proj', 'lang_model.model.layers.16.mlp.down_proj', 'lang_model.model.layers.17.self_attn.o_proj', 'lang_model.model.layers.17.self_attn.qkv_proj', 'lang_model.model.layers.17.mlp.gate_up_proj', 'lang_model.model.layers.17.mlp.down_proj', 'lang_model.model.layers.18.self_attn.o_proj', 'lang_model.model.layers.18.self_attn.qkv_proj', 'lang_model.model.layers.18.mlp.gate_up_proj', 'lang_model.model.layers.18.mlp.down_proj', 'lang_model.model.layers.19.self_attn.o_proj', 'lang_model.model.layers.19.self_attn.qkv_proj', 'lang_model.model.layers.19.mlp.gate_up_proj', 'lang_model.model.layers.19.mlp.down_proj', 'lang_model.model.layers.20.self_attn.o_proj', 'lang_model.model.layers.20.self_attn.qkv_proj', 'lang_model.model.layers.20.mlp.gate_up_proj', 'lang_model.model.layers.20.mlp.down_proj', 'lang_model.model.layers.21.self_attn.o_proj', 'lang_model.model.layers.21.self_attn.qkv_proj', 'lang_model.model.layers.21.mlp.gate_up_proj', 'lang_model.model.layers.21.mlp.down_proj', 'lang_model.model.layers.22.self_attn.o_proj', 'lang_model.model.layers.22.self_attn.qkv_proj', 'lang_model.model.layers.22.mlp.gate_up_proj', 'lang_model.model.layers.22.mlp.down_proj', 'lang_model.model.layers.23.self_attn.o_proj', 'lang_model.model.layers.23.self_attn.qkv_proj', 'lang_model.model.layers.23.mlp.gate_up_proj', 'lang_model.model.layers.23.mlp.down_proj', 'lang_model.model.layers.24.self_attn.o_proj', 'lang_model.model.layers.24.self_attn.qkv_proj', 'lang_model.model.layers.24.mlp.gate_up_proj', 'lang_model.model.layers.24.mlp.down_proj', 'lang_model.model.layers.25.self_attn.o_proj', 'lang_model.model.layers.25.self_attn.qkv_proj', 'lang_model.model.layers.25.mlp.gate_up_proj', 'lang_model.model.layers.25.mlp.down_proj', 'lang_model.model.layers.26.self_attn.o_proj', 'lang_model.model.layers.26.self_attn.qkv_proj', 'lang_model.model.layers.26.mlp.gate_up_proj', 'lang_model.model.layers.26.mlp.down_proj', 'lang_model.model.layers.27.self_attn.o_proj', 'lang_model.model.layers.27.self_attn.qkv_proj', 'lang_model.model.layers.27.mlp.gate_up_proj', 'lang_model.model.layers.27.mlp.down_proj', 'lang_model.model.layers.28.self_attn.o_proj', 'lang_model.model.layers.28.self_attn.qkv_proj', 'lang_model.model.layers.28.mlp.gate_up_proj', 'lang_model.model.layers.28.mlp.down_proj', 'lang_model.model.layers.29.self_attn.o_proj', 'lang_model.model.layers.29.self_attn.qkv_proj', 'lang_model.model.layers.29.mlp.gate_up_proj', 'lang_model.model.layers.29.mlp.down_proj', 'lang_model.model.layers.30.self_attn.o_proj', 'lang_model.model.layers.30.self_attn.qkv_proj', 'lang_model.model.layers.30.mlp.gate_up_proj', 'lang_model.model.layers.30.mlp.down_proj', 'lang_model.model.layers.31.self_attn.o_proj', 'lang_model.model.layers.31.self_attn.qkv_proj', 'lang_model.model.layers.31.mlp.gate_up_proj', 'lang_model.model.layers.31.mlp.down_proj', 'lang_model.lm_head.additional_fc']
trainable params: 27,310,128 || all params: 4,386,567,347 || trainable%: 0.6226
================== Data mixture config ===================
{'data_path': {'/workspace/detail_23k.json': 2000}}
==========================================================
dataloader [[384, 768], [768, 384], [768, 768], [1152, 384], [384, 1152]]
Total training steps: 250
DistributedDataParallel(
  (module): PeftModel(
    (base_model): LoraModel(
      (model): XGenMMPerceiver(
        (vision_encoder): SiglipVisionTransformer(
          (embeddings): SiglipVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)
            (position_embedding): Embedding(729, 1152)
          )
          (encoder): SiglipEncoder(
            (layers): ModuleList(
              (0-26): 27 x SiglipEncoderLayer(
                (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
                (self_attn): SiglipAttention(
                  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)
                  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)
                  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)
                  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)
                )
                (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
                (mlp): SiglipMLP(
                  (activation_fn): PytorchGELUTanh()
                  (fc1): Linear(in_features=1152, out_features=4304, bias=True)
                  (fc2): Linear(in_features=4304, out_features=1152, bias=True)
                )
              )
            )
          )
          (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
          (head): SiglipMultiheadAttentionPoolingHead(
            (attention): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1152, out_features=1152, bias=True)
            )
            (layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
            (mlp): SiglipMLP(
              (activation_fn): PytorchGELUTanh()
              (fc1): Linear(in_features=1152, out_features=4304, bias=True)
              (fc2): Linear(in_features=4304, out_features=1152, bias=True)
            )
          )
        )
        (vision_tokenizer): CheckpointWrapper(
          (_checkpoint_wrapped_module): PerceiverResampler(
            (projection): lora.Linear(
              (base_layer): Linear(in_features=1152, out_features=3072, bias=True)
              (lora_dropout): ModuleDict(
                (default): Dropout(p=0.05, inplace=False)
              )
              (lora_A): ModuleDict(
                (default): Linear(in_features=1152, out_features=16, bias=False)
              )
              (lora_B): ModuleDict(
                (default): Linear(in_features=16, out_features=3072, bias=False)
              )
              (lora_embedding_A): ParameterDict()
              (lora_embedding_B): ParameterDict()
              (lora_magnitude_vector): ModuleDict()
            )
            (layers): ModuleList(
              (0-5): 6 x ModuleList(
                (0): PerceiverAttention(
                  (norm_media): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)
                  (norm_latents): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)
                  (to_q): lora.Linear(
                    (base_layer): Linear(in_features=1152, out_features=1536, bias=False)
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.05, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1152, out_features=16, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=16, out_features=1536, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                    (lora_magnitude_vector): ModuleDict()
                  )
                  (to_kv): lora.Linear(
                    (base_layer): Linear(in_features=1152, out_features=3072, bias=False)
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.05, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1152, out_features=16, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=16, out_features=3072, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                    (lora_magnitude_vector): ModuleDict()
                  )
                  (to_out): lora.Linear(
                    (base_layer): Linear(in_features=1536, out_features=1152, bias=False)
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.05, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1536, out_features=16, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=16, out_features=1152, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                    (lora_magnitude_vector): ModuleDict()
                  )
                )
                (1): Sequential(
                  (0): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)
                  (1): lora.Linear(
                    (base_layer): Linear(in_features=1152, out_features=4608, bias=False)
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.05, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1152, out_features=16, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=16, out_features=4608, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                    (lora_magnitude_vector): ModuleDict()
                  )
                  (2): GELU(approximate='none')
                  (3): lora.Linear(
                    (base_layer): Linear(in_features=4608, out_features=1152, bias=False)
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.05, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=4608, out_features=16, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=16, out_features=1152, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                    (lora_magnitude_vector): ModuleDict()
                  )
                )
              )
            )
            (norm): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)
          )
        )
        (lang_model): Phi3ForCausalLM(
          (model): Phi3Model(
            (embed_tokens): DecoupledEmbedding(
              num_original_embeddings=32012, num_additional_embeddings=3, embedding_dim=3072, partially_freeze=True
              (additional_embedding): Embedding(3, 3072)
            )
            (embed_dropout): Dropout(p=0.0, inplace=False)
            (layers): ModuleList(
              (0-31): 32 x CheckpointWrapper(
                (_checkpoint_wrapped_module): Phi3DecoderLayer(
                  (self_attn): Phi3Attention(
                    (o_proj): lora.Linear(
                      (base_layer): Linear(in_features=3072, out_features=3072, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.05, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): Linear(in_features=3072, out_features=16, bias=False)
                      )
                      (lora_B): ModuleDict(
                        (default): Linear(in_features=16, out_features=3072, bias=False)
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                      (lora_magnitude_vector): ModuleDict()
                    )
                    (qkv_proj): lora.Linear(
                      (base_layer): Linear(in_features=3072, out_features=9216, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.05, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): Linear(in_features=3072, out_features=16, bias=False)
                      )
                      (lora_B): ModuleDict(
                        (default): Linear(in_features=16, out_features=9216, bias=False)
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                      (lora_magnitude_vector): ModuleDict()
                    )
                    (rotary_emb): Phi3RotaryEmbedding()
                  )
                  (mlp): Phi3MLP(
                    (gate_up_proj): lora.Linear(
                      (base_layer): Linear(in_features=3072, out_features=16384, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.05, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): Linear(in_features=3072, out_features=16, bias=False)
                      )
                      (lora_B): ModuleDict(
                        (default): Linear(in_features=16, out_features=16384, bias=False)
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                      (lora_magnitude_vector): ModuleDict()
                    )
                    (down_proj): lora.Linear(
                      (base_layer): Linear(in_features=8192, out_features=3072, bias=False)
                      (lora_dropout): ModuleDict(
                        (default): Dropout(p=0.05, inplace=False)
                      )
                      (lora_A): ModuleDict(
                        (default): Linear(in_features=8192, out_features=16, bias=False)
                      )
                      (lora_B): ModuleDict(
                        (default): Linear(in_features=16, out_features=3072, bias=False)
                      )
                      (lora_embedding_A): ParameterDict()
                      (lora_embedding_B): ParameterDict()
                      (lora_magnitude_vector): ModuleDict()
                    )
                    (activation_fn): SiLU()
                  )
                  (input_layernorm): Phi3RMSNorm()
                  (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  (post_attention_layernorm): Phi3RMSNorm()
                )
              )
            )
            (norm): Phi3RMSNorm()
          )
          (lm_head): DecoupledLinear(
            in_features=3072, out_features=32012, additional_out_features=3, bias=True, partially_freeze=True
            (additional_fc): lora.Linear(
              (base_layer): Linear(in_features=3072, out_features=3, bias=True)
              (lora_dropout): ModuleDict(
                (default): Dropout(p=0.05, inplace=False)
              )
              (lora_A): ModuleDict(
                (default): Linear(in_features=3072, out_features=16, bias=False)
              )
              (lora_B): ModuleDict(
                (default): Linear(in_features=16, out_features=3, bias=False)
              )
              (lora_embedding_A): ParameterDict()
              (lora_embedding_B): ParameterDict()
              (lora_magnitude_vector): ModuleDict()
            )
          )
        )
      )
    )
  )
)
Start running training on rank 0.
  0%|          | 0/2000 [00:00<?, ?it/s]Forward Call
images.shape=torch.Size([1, 1, 5, 3, 384, 384]) image_size=[[640, 480]] input_ids.shape=torch.Size([1, 220]) attention_mask.shape=torch.Size([1, 220]) labels.shape=torch.Size([1, 220])

You are not running the flash-attention implementation, expect numerical differences.
  0%|          | 1/2000 [19:34<652:06:28, 1174.38s/it]Forward Call
images.shape=torch.Size([1, 1, 5, 3, 384, 384]) image_size=[[640, 480]] input_ids.shape=torch.Size([1, 189]) attention_mask.shape=torch.Size([1, 189]) labels.shape=torch.Size([1, 189])

  0%|          | 1/2000 [19:34<652:06:43, 1174.39s/it]
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/debugpy/_vendored/pydevd/pydevd.py", line 3489, in <module>
    main()
  File "/usr/local/lib/python3.10/dist-packages/debugpy/_vendored/pydevd/pydevd.py", line 3482, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File "/usr/local/lib/python3.10/dist-packages/debugpy/_vendored/pydevd/pydevd.py", line 2510, in run
    return self._exec(is_module, entry_point_fn, module_name, file, globals, locals)
  File "/usr/local/lib/python3.10/dist-packages/debugpy/_vendored/pydevd/pydevd.py", line 2517, in _exec
    globals = pydevd_runpy.run_path(file, globals, '__main__')
  File "/usr/local/lib/python3.10/dist-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py", line 321, in run_path
    return _run_module_code(code, init_globals, run_name,
  File "/usr/local/lib/python3.10/dist-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py", line 135, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File "/usr/local/lib/python3.10/dist-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py", line 124, in _run_code
    exec(code, run_globals)
  File "open_flamingo/train/instruction_finetune.py", line 501, in <module>
    main()
  File "open_flamingo/train/instruction_finetune.py", line 483, in main
    finetune_one_epoch(
  File "/workspace/LAVIS/open_flamingo/train/train_utils.py", line 301, in finetune_one_epoch
    loss = compute_loss_fn(
  File "/workspace/LAVIS/open_flamingo/train/losses.py", line 111, in __call__
    loss = model(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py", line 1515, in forward
    inputs, kwargs = self._pre_forward(*inputs, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py", line 1409, in _pre_forward
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 0: 318 319
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
