================== Parsed Arguments ===================
anyres_grids: [(1, 2), (2, 1), (2, 2), (3, 1), (1, 3)]
anyres_patch_sampling: True
batch_size: 1
checkpoint_steps: 500
conv_template_name: phi_3
cpu_offload_gradients: False
cross_attn_every_n_layers: 1
data_path: qvhighlights
data_sampler_group_by_length: True
delete_previous_checkpoint: False
dist_backend: nccl
dist_url: env://
dryrun: False
fsdp: False
fsdp_sharding_strategy: full
gradient_accumulation_steps: 16
gradient_checkpointing: True
horovod: False
image_aspect_ratio: anyres
is_multimodal: True
learning_rate: 2e-05
lm_path: microsoft/Phi-3-mini-4k-instruct
local_rank: 0
logging_steps: 50
lora: False
loss: supervised_finetune
lr_scheduler: cosine
mm_use_im_start_end: False
model_family: xgenmm_v1
no_save_optim_state: True
no_set_device_rank: False
num_epochs: 2
num_vision_tokens: 128
offline: False
precision: amp_bf16
pretrained: /workspace/moment-retrieval-with-llm/base_model_weight/xgen-mm-phi3-mini-base-r-v1.5.pt
pretrained_vision_tokenizer: None
report_to_wandb: False
resume_from_checkpoint: None
run_name: finetune-xgenmmv1-phi3_4k_instruct-qvhighlights-lora-v1
save_checkpoints_to_wandb: False
seed: 42
tokenizer_path: microsoft/Phi-3-mini-4k-instruct
unfreeze_vision_encoder: False
use_flash_attention_2: True
vision_encoder_path: google/siglip-so400m-patch14-384
vision_encoder_precision: fp32
vision_encoder_pretrained: google
wandb_entity: None
wandb_project: None
warmup_steps: 1085
weight_decay: 0.0
workers: 2
=======================================================
Initializing distributed training with 1 GPUs.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 23.37it/s]
xgenmm_v1 model initialized with 3,931,031,619 trainable parameters
==========Trainable Parameters
Vision encoder: 0 trainable parameters
Vision tokenizer: 109,901,568 trainable parameters
Language model: 3,821,130,051 trainable parameters
==========Total Parameters
Vision encoder: 428,225,600 parameters
Vision tokenizer: 109,901,568 parameters
Language model: 3,821,130,051 parameters
==========
Loading checkpoint from /workspace/moment-retrieval-with-llm/base_model_weight/xgen-mm-phi3-mini-base-r-v1.5.pt
Missing keys: []
Unexpected keys: []
Finished loading checkpoint...
Wrapping model in LoRA
================== Data mixture config ===================
qvhighlights
{
    "train": {
        "annotations": {
            "../datasets/qvhighlights/annotations/processed/highlight_train_release.json": 1000
        },
        "videos": "../datasets/qvhighlights/videos/processed"
    },
    "val": {
        "annotations": {
            "../datasets/qvhighlights/annotations/processed/highlight_val_release.json": 200
        },
        "videos": "../datasets/qvhighlights/videos/processed"
    }
}
==========================================================
Total training steps: 124
DistributedDataParallel(
  (module): XGenMMPerceiver(
    (vision_encoder): SiglipVisionTransformer(
      (embeddings): SiglipVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)
        (position_embedding): Embedding(729, 1152)
      )
      (encoder): SiglipEncoder(
        (layers): ModuleList(
          (0-26): 27 x SiglipEncoderLayer(
            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
            (self_attn): SiglipAttention(
              (k_proj): Linear(in_features=1152, out_features=1152, bias=True)
              (v_proj): Linear(in_features=1152, out_features=1152, bias=True)
              (q_proj): Linear(in_features=1152, out_features=1152, bias=True)
              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)
            )
            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
            (mlp): SiglipMLP(
              (activation_fn): PytorchGELUTanh()
              (fc1): Linear(in_features=1152, out_features=4304, bias=True)
              (fc2): Linear(in_features=4304, out_features=1152, bias=True)
            )
          )
        )
      )
      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
      (head): SiglipMultiheadAttentionPoolingHead(
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1152, out_features=1152, bias=True)
        )
        (layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
        (mlp): SiglipMLP(
          (activation_fn): PytorchGELUTanh()
          (fc1): Linear(in_features=1152, out_features=4304, bias=True)
          (fc2): Linear(in_features=4304, out_features=1152, bias=True)
        )
      )
    )
    (vision_tokenizer): CheckpointWrapper(
      (_checkpoint_wrapped_module): PerceiverResampler(
        (projection): Linear(in_features=1152, out_features=3072, bias=True)
        (layers): ModuleList(
          (0-5): 6 x ModuleList(
            (0): PerceiverAttention(
              (norm_media): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)
              (norm_latents): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=1152, out_features=1536, bias=False)
              (to_kv): Linear(in_features=1152, out_features=3072, bias=False)
              (to_out): Linear(in_features=1536, out_features=1152, bias=False)
            )
            (1): Sequential(
              (0): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)
              (1): Linear(in_features=1152, out_features=4608, bias=False)
              (2): GELU(approximate='none')
              (3): Linear(in_features=4608, out_features=1152, bias=False)
            )
          )
        )
        (norm): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)
      )
    )
    (lang_model): Phi3ForCausalLM(
      (model): Phi3Model(
        (embed_tokens): DecoupledEmbedding(
          num_original_embeddings=32012, num_additional_embeddings=3, embedding_dim=3072, partially_freeze=False
          (additional_embedding): Embedding(3, 3072)
        )
        (layers): ModuleList(
          (0-31): 32 x CheckpointWrapper(
            (_checkpoint_wrapped_module): Phi3DecoderLayer(
              (self_attn): Phi3Attention(
                (o_proj): Linear(in_features=3072, out_features=3072, bias=False)
                (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)
              )
              (mlp): Phi3MLP(
                (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)
                (down_proj): Linear(in_features=8192, out_features=3072, bias=False)
                (activation_fn): SiLU()
              )
              (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
              (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
              (resid_attn_dropout): Dropout(p=0.0, inplace=False)
              (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (norm): Phi3RMSNorm((3072,), eps=1e-05)
        (rotary_emb): Phi3RotaryEmbedding()
      )
      (lm_head): DecoupledLinear(
        in_features=3072, out_features=32012, additional_out_features=3, bias=True, partially_freeze=False
        (additional_fc): Linear(in_features=3072, out_features=3, bias=True)
      )
    )
  )
)
Start running training on rank 0.
  0%|          | 0/2000 [00:00<?, ?it/s][W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 0/2000 [00:04<?, ?it/s]
Traceback (most recent call last):
  File "/workspace/moment-retrieval-with-llm/LAVIS/open_flamingo/train/instruction_finetune.py", line 299, in <module>
    main()
  File "/workspace/moment-retrieval-with-llm/LAVIS/open_flamingo/train/instruction_finetune.py", line 281, in main
    finetune_one_epoch(
  File "/workspace/moment-retrieval-with-llm/LAVIS/open_flamingo/train/train_utils.py", line 141, in finetune_one_epoch
    divided_loss.backward()
  File "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacty of 23.57 GiB of which 55.38 MiB is free. Process 1029795 has 23.51 GiB memory in use. Of the allocated memory 22.37 GiB is allocated by PyTorch, and 737.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2025-05-08 17:27:44,726] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 29898) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 810, in <module>
    main()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
open_flamingo/train/instruction_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-05-08_17:27:44
  host      : 6d094fb98fde
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 29898)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
